{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from livelossplot import PlotLosses\n",
    "from collections import deque\n",
    "from envs.GraphNavEnv.graph_navigation_env import GraphNavEnv\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:30.704810200Z",
     "start_time": "2024-03-15T14:29:30.532759200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:30.996346400Z",
     "start_time": "2024-03-15T14:29:30.685277900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from envs.GraphEnv.impnode import ImpnodeEnv\n",
    "import DQN_agent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:31.200818500Z",
     "start_time": "2024-03-15T14:29:30.919340300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:31.284236600Z",
     "start_time": "2024-03-15T14:29:31.138813700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# to initialize the replay buffer with some random interactions\n",
    "\n",
    "def fill_memory(env, agent):\n",
    "    for _ in range(NUM_MEM_FILL_EPS):\n",
    "        N_STEP = 5\n",
    "        state_history, action_history, reward_history = [], [], []\n",
    "        done = False\n",
    "        state, info = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample(mask=info['node_action_mask']) # samples random action\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            state_history.append(state)\n",
    "            action_history.append(action)\n",
    "            reward_history.append(reward)\n",
    "\n",
    "            if len(state_history) >= N_STEP:\n",
    "                n_step_states = state_history[-N_STEP]\n",
    "                n_step_actions = action_history[-N_STEP]\n",
    "                n_step_rewards = reward_history[-N_STEP:]\n",
    "\n",
    "                # Calculate n-step return\n",
    "                n_step_return = sum(reward * (agent.discount ** i) for i, reward in enumerate(n_step_rewards))\n",
    "                agent.memory.store(\n",
    "                    state=n_step_states,\n",
    "                    action=n_step_actions,\n",
    "                    next_state=next_state,\n",
    "                    reward=n_step_return,\n",
    "                    done=done\n",
    "                )\n",
    "            state = next_state\n",
    "            #agent.memory.store(state=state, action=action, next_state=next_state, reward=reward, done=done)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:31.405666Z",
     "start_time": "2024-03-15T14:29:31.278898500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# trains the agent and plots the associated moving average rewards and epsilon values in real-time\n",
    "\n",
    "def train_loop(env, agent, results_basepath):\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "    liveloss = PlotLosses()\n",
    "    logs = {}\n",
    "\n",
    "    last_100_rewards = deque([], maxlen=50)\n",
    "\n",
    "    reward_history = []\n",
    "    epsilon_history = []\n",
    "\n",
    "    step_cnt = 0\n",
    "    best_score = -np.inf\n",
    "\n",
    "    N_STEP = 5\n",
    "\n",
    "    for ep_cnt in range(NUM_TRAIN_EPS):\n",
    "\n",
    "        #logs['train epsilon'] = agent.epsilon # to plot current epsilon value\n",
    "        state_history, action_history, reward_history = [], [], []\n",
    "        done = False\n",
    "        state, info = env.reset()\n",
    "        ep_score = 0\n",
    "        while not done:\n",
    "            mask = info['node_action_mask']\n",
    "            action = agent.select_action(state, mask)\n",
    "\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            state_history.append(state)\n",
    "            action_history.append(action)\n",
    "            reward_history.append(reward)\n",
    "\n",
    "            if len(state_history) >= N_STEP:\n",
    "                n_step_states = state_history[-N_STEP]\n",
    "                n_step_actions = action_history[-N_STEP]\n",
    "                n_step_rewards = reward_history[-N_STEP:]\n",
    "\n",
    "                # Calculate n-step return\n",
    "                n_step_return = sum(reward * (agent.discount ** i) for i, reward in enumerate(n_step_rewards))\n",
    "                agent.memory.store(\n",
    "                    state=n_step_states,\n",
    "                    action=n_step_actions,\n",
    "                    next_state=next_state,\n",
    "                    reward=n_step_return,\n",
    "                    done=done\n",
    "                )\n",
    "                agent.learn(BATCHSIZE)\n",
    "\n",
    "                if step_cnt % UPDATE_FREQUENCY == 0:\n",
    "                    agent.update_target_net()\n",
    "\n",
    "                step_cnt += 1\n",
    "\n",
    "            state = next_state\n",
    "            ep_score += reward\n",
    "            ##########\n",
    "            #agent.memory.store(state=state, action=action, next_state=next_state, reward=reward, done=done)\n",
    "            #agent.learn(BATCHSIZE)\n",
    "\n",
    "            # if step_cnt % UPDATE_FREQUENCY == 0:\n",
    "            #     agent.update_target_net()\n",
    "\n",
    "            # state = next_state\n",
    "            # ep_score += reward\n",
    "            # step_cnt += 1\n",
    "\n",
    "        agent.update_epsilon()\n",
    "\n",
    "        last_100_rewards.append(ep_score)\n",
    "        current_avg_score = np.mean(last_100_rewards) # get average of last 100 scores\n",
    "        logs['train avg score'] = current_avg_score\n",
    "\n",
    "        reward_history.append(ep_score)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "\n",
    "        if ep_cnt % 300 == 0:\n",
    "            agent.data = True\n",
    "            val_score_history = []\n",
    "            for ep in range(100):\n",
    "                ep_score = 0\n",
    "                done = False\n",
    "                state, info = env.reset(ep)\n",
    "                while not done:\n",
    "                    mask = info['node_action_mask']\n",
    "                    action = agent.select_action(state, mask)\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    ep_score += reward\n",
    "                    state = next_state\n",
    "\n",
    "                # track reward history only while running locally\n",
    "\n",
    "                val_score_history.append(ep_score)\n",
    "                agent.data = False\n",
    "            val_score = np.average(val_score_history)\n",
    "\n",
    "            early_stopping(val_score, agent)\n",
    "            logs['val avg score'] = val_score\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            if val_score >= best_score:\n",
    "                #agent.save_model('{}/dqn_model'.format(results_basepath))\n",
    "                agent.save_model('{}/model.pt'.format(results_basepath))\n",
    "                best_score = val_score\n",
    "\n",
    "        # update the plots in real-time\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "\n",
    "    # store the reward and epsilon history that was tracked while running locally\n",
    "\n",
    "    with open('{}/train_reward_history.pkl'.format(results_basepath), 'wb') as f:\n",
    "        pickle.dump(reward_history, f)\n",
    "\n",
    "    with open('{}/train_epsilon_history.pkl'.format(results_basepath), 'wb') as f:\n",
    "        pickle.dump(epsilon_history, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:31.531637700Z",
     "start_time": "2024-03-15T14:29:31.411665400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# variables for training the agent\n",
    "\n",
    "NUM_TRAIN_EPS = 10 #1000 number training episodes to run\n",
    "NUM_MEM_FILL_EPS = 10 #10 number of episodes to run to initialize the memory\n",
    "\n",
    "DISCOUNT = 0.99 # gamma used for computing return\n",
    "\n",
    "BATCHSIZE = 64 # number of transitions to sample from replay buffer for each learn step\n",
    "MEMORY_CAPACITY = 50 # size of the memory buffer\n",
    "UPDATE_FREQUENCY = 10 # number of interactions after which the target buffer is updated\n",
    "\n",
    "EPS_MAX = 1.0 # initial epsilon value\n",
    "EPS_MIN = 0.05 # final epsilon value\n",
    "EPS_STEP = 100 # amount by which epsilon is decayed at each episode\n",
    "\n",
    "LR = 0.01 # learning rate for the network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:31.656556100Z",
     "start_time": "2024-03-15T14:29:31.533117Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# create folder for storing the model and other files\n",
    "# graph_size, layer_iterations, embedding_dim, NUM_TRAIN_EPS,NUM_MEM_FILL_EPS,DISCOUNT,BATCHSIZE,MEMORY_CAPACITY, UPDATE_FREQUENCY,EPS_MAX,EPS_MIN, EPS_STEP,LR\n",
    "results_basepath_train = \"results/trial_model_newest\".format(\n",
    "                        NUM_TRAIN_EPS,NUM_MEM_FILL_EPS,DISCOUNT,BATCHSIZE,MEMORY_CAPACITY, UPDATE_FREQUENCY,EPS_MAX,EPS_MIN, EPS_STEP,LR)\n",
    "os.makedirs(results_basepath_train, exist_ok=True)\n",
    "\n",
    "subdir = 'data/30-50'\n",
    "data_path = Path.cwd()/subdir\n",
    "\n",
    "seed = None\n",
    "#env_train = ImpnodeEnv(anc='dw_nd', ba_nodes=(15, 25), ba_edges = 4,max_removed_nodes = 10, seed=seed, render_option=False, data= False,data_path=data_path, train_mode=True)\n",
    "env_train = GraphNavEnv(fix_random_graphs=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:29:31.813144800Z",
     "start_time": "2024-03-15T14:29:31.659556200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# create the dqn_agent\n",
    "dqn_agent_train = DQN_agent.DQNAgent(device,\n",
    "                                     #env_train.observation_space.shape[0],\n",
    "                                     2,#5,\n",
    "                                     env_train.action_space.n,\n",
    "                                     discount=DISCOUNT,\n",
    "                                     eps_max=EPS_MAX,\n",
    "                                     eps_min=EPS_MIN,\n",
    "                                     eps_step=EPS_STEP,\n",
    "                                     memory_capacity=MEMORY_CAPACITY,\n",
    "                                     lr=LR,\n",
    "                                     train_mode=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:31:01.997868500Z",
     "start_time": "2024-03-15T14:31:01.802241900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (35x5 and 2x46)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m fill_memory(env_train, dqn_agent_train)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# train the agent\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdqn_agent_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults_basepath_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[16], line 89\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(env, agent, results_basepath)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m     88\u001B[0m     mask \u001B[38;5;241m=\u001B[39m info[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnode_action_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 89\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m     next_state, reward, done, truncated, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     91\u001B[0m     ep_score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "File \u001B[1;32mC:\\rituja_git\\ma-rituja-pardhi\\DQN\\DQN_agent.py:136\u001B[0m, in \u001B[0;36mDQNAgent.select_action\u001B[1;34m(self, state, mask)\u001B[0m\n\u001B[0;32m    134\u001B[0m pyg_state \u001B[38;5;241m=\u001B[39m torch_geometric\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mfrom_networkx(new_state)\n\u001B[0;32m    135\u001B[0m batch_of_state \u001B[38;5;241m=\u001B[39m torch_geometric\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mBatch\u001B[38;5;241m.\u001B[39mfrom_data_list([pyg_state])  \u001B[38;5;66;03m# new\u001B[39;00m\n\u001B[1;32m--> 136\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_net\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_of_state\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m#.squeeze(1)\u001B[39;00m\n\u001B[0;32m    137\u001B[0m action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmul(action, torch\u001B[38;5;241m.\u001B[39mtensor(mask))  \u001B[38;5;66;03m# disable invalid nodes\u001B[39;00m\n\u001B[0;32m    138\u001B[0m a \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(action)\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mC:\\rituja_git\\ma-rituja-pardhi\\DQN\\model.py:49\u001B[0m, in \u001B[0;36mDQNNet.forward\u001B[1;34m(self, data, embedding)\u001B[0m\n\u001B[0;32m     46\u001B[0m x, edge_index \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mfeatures\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32), data\u001B[38;5;241m.\u001B[39medge_index\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# x, edge_index, edge_attr = data.x.to(torch.float32), data.edge_index, data.edge_attr.to(torch.float32)\u001B[39;00m\n\u001B[1;32m---> 49\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     50\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m/\u001B[39m x\u001B[38;5;241m.\u001B[39mnorm(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m):\n",
      "File \u001B[1;32m~\\.conda\\envs\\impnode\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\.conda\\envs\\impnode\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (35x5 and 2x46)"
     ]
    }
   ],
   "source": [
    "# initialise the memory\n",
    "fill_memory(env_train, dqn_agent_train)\n",
    "\n",
    "# train the agent\n",
    "train_loop(env_train, dqn_agent_train, results_basepath_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T14:31:02.804540300Z",
     "start_time": "2024-03-15T14:31:02.000868600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tests the agent through interactions with the environment and plots the associated rewards in real-time\n",
    "\n",
    "def test_loop(env, agent, results_basepath):\n",
    "    liveloss = PlotLosses()\n",
    "    logs = {}\n",
    "\n",
    "    reward_history = []\n",
    "    actions = []\n",
    "    for ep in range(NUM_TEST_EPS):\n",
    "        ep_score = 0\n",
    "        done = False\n",
    "        state, info = env.reset(ep)\n",
    "        while not done:\n",
    "            mask = info['node_actio n_mask']\n",
    "            action = agent.select_action(state, mask)\n",
    "            actions.append(action)\n",
    "            print(action)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            ep_score += reward\n",
    "            state = next_state\n",
    "\n",
    "        # track reward history only while running locally\n",
    "\n",
    "            reward_history.append(reward)\n",
    "\n",
    "        # update the plot in real-time\n",
    "            logs['test score'] = reward\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()\n",
    "    return actions\n",
    "\n",
    "    # store the reward that was tracked while running locally\n",
    "\n",
    "    with open('{}/test_reward_history.pkl'.format(results_basepath), 'wb') as f:\n",
    "        pickle.dump(reward_history, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T14:29:33.594850100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# variables for testing the agent\n",
    "# location where the model is stored and the name of the associated environment\n",
    "\n",
    "RESULTS_BASEPATH_TEST = 'results/new_30-50_traineps50000_epsmax1.0_epsmin0.05_epsstep10000_batchsize64_treps50000_memeps1000_memcap500000_gseedFalse'\n",
    "\n",
    "NUM_TEST_EPS = 1 # number of test episodes to run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T14:29:33.598850600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create test environment and set associated seed\n",
    "\n",
    "seed = 1\n",
    "env_test = ImpnodeEnv(ba_nodes=random.randint(15, 25) * 2 ,ba_edges = 4,max_removed_nodes = 3, seed=seed, render_option=False, data=False, train_mode=False)\n",
    "\n",
    "# create the dqn agent with the stored weights\n",
    "dqn_agent_test = DQN_agent.DQNAgent(device=device,\n",
    "                          state_size=5,#env_test.observation_space.shape[0],\n",
    "                          action_size=env_test.action_space.n,\n",
    "                          discount=0.0,\n",
    "                          eps_max=0.0,\n",
    "                          eps_min=0.0,\n",
    "                          eps_step=0.0,\n",
    "                          memory_capacity=0,\n",
    "                          lr=0,\n",
    "                          train_mode=False)\n",
    "dqn_agent_test.load_model('{}/dqn_model'.format(RESULTS_BASEPATH_TEST))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T14:29:33.601869600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test the agent\n",
    "actions = test_loop(env=env_test,\n",
    "          agent=dqn_agent_test,\n",
    "          results_basepath=RESULTS_BASEPATH_TEST)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T14:29:33.605279300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T14:29:33.607300900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T14:29:33.610302300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
